{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "This includes import files and basic cleaning required for Principal Component Analysis. We remove the categorical variables and include only the quantitative variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [('latitude'), ('longitude'), ('depth'), ('Gap'), ('dmin'), ('rms'), \n",
    "    ('horizontalError'), ('depthError'), ('magError'), ('magNST')]\n",
    "X = pd.read_csv('Xpca.csv', names=cols)  \n",
    "Y = pd.read_csv('Y_query.csv')\n",
    "#Y.astype(float)\n",
    "#X.dtypes\n",
    "X[cols] = X[cols].apply(pd.to_numeric, axis=1, errors='coerce')\n",
    "X = X.drop(X.index[0])\n",
    "#X.head()\n",
    "#X.dtypes\n",
    "#X.fillna(0)\n",
    "\n",
    "all_inf_or_nan = X.isin([np.inf, -np.inf, np.nan]).all(axis='columns')\n",
    "#X[~all_inf_or_nan]\n",
    "\n",
    "X = X.fillna(value=0) # null => 0\n",
    "#assert that there are no missing values\n",
    "assert pd.notnull(X).all().all()\n",
    "#X.info(null_counts=True)\n",
    "#Y.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split testing data from training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)  \n",
    "#print(X)\n",
    "#X.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing the input variables to have uniformity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# Fit on training set only\n",
    "scaler.fit(X_train)\n",
    "# Apply transform to both the training set and the test set\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is performed using Sklearn package. Shape of transformed variable is also printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()  \n",
    "X_train = pca.fit_transform(X_train)  \n",
    "X_test = pca.transform(X_test)  \n",
    "#print(pca.n_components_)\n",
    "#print(fit.components_)\n",
    "#print(X.feature_names)\n",
    "#print(X.shape)\n",
    "#print(X_test.shape)\n",
    "#print(X_test)\n",
    "#print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explained variance and explained variance ratio are calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23038235 0.17982504 0.13206089 0.10831807 0.09987504 0.08549434\n",
      " 0.05002188 0.04506632 0.03783222 0.03112384]\n",
      "[2.34254575 1.82847505 1.34280549 1.1013866  1.01553725 0.86931314\n",
      " 0.50862643 0.45823787 0.38468096 0.31646965]\n"
     ]
    }
   ],
   "source": [
    "#ex_variance = np.var(X_test, axis=0)\n",
    "#ex_variance_ratio = ex_variance / np.sum(ex_variance)\n",
    "#print(ex_variance_ratio)\n",
    "#print(ex_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA Components and expalined variance are found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access values and vectors\n",
    "#print(pca.components_)\n",
    "#print(pca.explained_variance_)\n",
    "# transform data\n",
    "#B = pca.transform(X)\n",
    "#print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Principal Components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "#print(pca.n_components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is Centerized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the centerized data: (8582, 10)\n"
     ]
    }
   ],
   "source": [
    "X_tilde = X - np.mean(X, axis=0)  \n",
    "#X_tilde = X - np.mean(X, axis=1)\n",
    "\n",
    "#assert X.shape == X_tilde.shape\n",
    "print('Shape of the centerized data:', X_tilde.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance matrix, Eigen Vectors, U Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC1 explains 0.5570657818007323% of the total variance\n",
      "PC2 explains 0.2884794436333058% of the total variance\n",
      "First 100 PCs explains 0.9999999999999998% of the total variance\n",
      "\n",
      "Shape of U:\n",
      " (8582, 10)\n",
      "First 5 elements of first column of U:\n",
      " [ 0.01579272  0.01369365  0.0377011  -0.00403405  0.00303744]\n",
      "First 5 elements of last column of U:\n",
      " [ 0.00357306  0.0001198  -0.01514307  0.00513204 -0.00469218]\n"
     ]
    }
   ],
   "source": [
    "# Covariance matrix\n",
    "covmat = np.dot(X_tilde.T, X_tilde) / X.shape[1]\n",
    "\n",
    "# Compute u'_i, which is stored in the variable v\n",
    "w, v = np.linalg.eig(covmat)\n",
    "\n",
    "# Compute u_i from u'_i, and store it in the variable U\n",
    "#U = np.dot(X_tilde,v)\n",
    "U = np.dot(X_tilde, v[0:v.shape[1]])\n",
    "           \n",
    "# Normalize u_i, i.e., each column of U\n",
    "U /= (np.linalg.norm(U, axis=0))\n",
    "\n",
    "# Evaluate eigenvalues\n",
    "ratios = w / np.sum(w)\n",
    "print('PC1 explains {}% of the total variance'.format(ratios[0]))\n",
    "print('PC2 explains {}% of the total variance'.format(ratios[1]))\n",
    "print('First 100 PCs explains {}% of the total variance\\n'.format(sum(ratios[:100])))\n",
    "\n",
    "# Evaluate U\n",
    "print('Shape of U:\\n', U.shape)\n",
    "print('First 5 elements of first column of U:\\n', U[:5, 0])\n",
    "print('First 5 elements of last column of U:\\n', U[:5, -1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
